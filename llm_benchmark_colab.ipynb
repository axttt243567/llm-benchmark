{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69112ccd",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57526c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate bitsandbytes torch sentencepiece protobuf\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "print(\"âœ“ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create benchmark directory in Drive\n",
    "import os\n",
    "DRIVE_RESULTS_DIR = '/content/drive/MyDrive/llm_benchmark_results'\n",
    "os.makedirs(DRIVE_RESULTS_DIR, exist_ok=True)\n",
    "print(f\"âœ“ Results will be saved to: {DRIVE_RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Login to HuggingFace for gated models (Llama, etc.)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "# Uncomment and run if you need access to gated models:\n",
    "# login(token=\"your_hf_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192caa4",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Define TransformersBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "class TransformersBackend:\n",
    "    \"\"\"\n",
    "    HuggingFace Transformers backend optimized for T4 GPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device: str = \"cuda\",\n",
    "        use_4bit: bool = False,\n",
    "        max_new_tokens: int = 512\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.use_4bit = use_4bit\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_name = None\n",
    "        \n",
    "        # Model mappings (Ollama-style -> HuggingFace)\n",
    "        self.model_mappings = {\n",
    "            # Qwen models (good for T4)\n",
    "            \"qwen2.5:0.5b\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "            \"qwen2.5:1.5b\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "            \"qwen2.5:3b\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "            \"qwen2.5:7b\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            # Gemma models\n",
    "            \"gemma2:2b\": \"google/gemma-2-2b-it\",\n",
    "            # Phi models\n",
    "            \"phi3.5:3.8b\": \"microsoft/Phi-3.5-mini-instruct\",\n",
    "            # SmolLM models (very efficient)\n",
    "            \"smollm2:135m\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    "            \"smollm2:360m\": \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "            \"smollm2:1.7b\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "            # Llama models (require HF login)\n",
    "            \"llama3.2:1b\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            \"llama3.2:3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        }\n",
    "    \n",
    "    def get_hf_name(self, model_name: str) -> str:\n",
    "        \"\"\"Convert to HuggingFace model name.\"\"\"\n",
    "        return self.model_mappings.get(model_name, model_name)\n",
    "    \n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"Load model with T4 optimizations.\"\"\"\n",
    "        hf_name = self.get_hf_name(model_name)\n",
    "        \n",
    "        if self.current_model_name == model_name:\n",
    "            return  # Already loaded\n",
    "        \n",
    "        # Clear previous model\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Loading model: {hf_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Prepare model kwargs\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "        \n",
    "        if self.use_4bit:\n",
    "            model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(hf_name, **model_kwargs)\n",
    "        self.model.eval()\n",
    "        self.current_model_name = model_name\n",
    "        \n",
    "        # Print memory usage\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"âœ“ Model loaded. VRAM: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "    def generate(self, model_name: str, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            self.load_model(model_name)\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "            \n",
    "            # Decode new tokens only\n",
    "            input_len = inputs['input_ids'].shape[1]\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][input_len:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"success\": True,\n",
    "                \"error_type\": None,\n",
    "                \"duration\": round(time.time() - start_time, 3)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": f\"ERROR: {str(e)}\",\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"TRANSFORMERS_ERROR\",\n",
    "                \"duration\": round(time.time() - start_time, 3)\n",
    "            }\n",
    "    \n",
    "    def get_available_models(self) -> List[str]:\n",
    "        return list(self.model_mappings.keys())\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            self.current_model_name = None\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"âœ“ Cache cleared\")\n",
    "\n",
    "# Initialize backend\n",
    "USE_4BIT = False  # @param {type:\"boolean\"}\n",
    "MAX_TOKENS = 512  # @param {type:\"integer\"}\n",
    "\n",
    "backend = TransformersBackend(use_4bit=USE_4BIT, max_new_tokens=MAX_TOKENS)\n",
    "print(f\"\\nâœ“ Backend initialized (4-bit: {USE_4BIT}, max_tokens: {MAX_TOKENS})\")\n",
    "print(f\"Available models: {backend.get_available_models()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409670e",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or upload benchmark data\n",
    "# Option 1: Clone from GitHub (replace with your repo)\n",
    "# !git clone https://github.com/axttt243567/llm-benchmark.git\n",
    "\n",
    "# Option 2: Upload data manually\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your JSON dataset files\n",
    "\n",
    "# Option 3: Use sample data (for testing)\n",
    "import json\n",
    "\n",
    "# Sample benchmark questions\n",
    "SAMPLE_QUESTIONS = [\n",
    "    {\n",
    "        \"id\": \"COMMON_001\",\n",
    "        \"domain\": \"common_sense\",\n",
    "        \"category\": \"everyday\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"test_for\": \"basic reasoning\",\n",
    "        \"prompt\": \"If it's raining outside and you want to stay dry, what should you bring with you?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"LOGIC_001\",\n",
    "        \"domain\": \"logic\",\n",
    "        \"category\": \"tricky\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"test_for\": \"logical reasoning\",\n",
    "        \"prompt\": \"A farmer has 17 sheep. All but 9 run away. How many sheep does the farmer have left?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"MATH_001\",\n",
    "        \"domain\": \"math\",\n",
    "        \"category\": \"algebra\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"test_for\": \"algebraic solving\",\n",
    "        \"prompt\": \"Solve for x: 2x + 5 = 13\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"COMMON_002\",\n",
    "        \"domain\": \"common_sense\",\n",
    "        \"category\": \"social\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"test_for\": \"social understanding\",\n",
    "        \"prompt\": \"Your friend just received bad news about a job application. What would be an appropriate thing to say?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"LOGIC_002\",\n",
    "        \"domain\": \"logic\",\n",
    "        \"category\": \"tricky\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"test_for\": \"attention to detail\",\n",
    "        \"prompt\": \"How many times can you subtract 5 from 25?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Loaded {len(SAMPLE_QUESTIONS)} sample questions\")\n",
    "print(\"\\nTo use your own data, upload JSON files or clone your benchmark repo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd1928",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e552a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_benchmark(model_name: str, questions: list, dataset_key: str = \"custom\"):\n",
    "    \"\"\"\n",
    "    Run benchmark on the given model and questions.\n",
    "    \"\"\"\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + hashlib.md5(str(time.time()).encode()).hexdigest()[:6]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸš€ Starting Benchmark\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset_key}\")\n",
    "    print(f\"Questions: {len(questions)}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    successful = 0\n",
    "    \n",
    "    for i, q in enumerate(tqdm(questions, desc=\"Processing\")):\n",
    "        prompt = q.get('prompt', '')\n",
    "        \n",
    "        # Generate response\n",
    "        result = backend.generate(model_name, prompt)\n",
    "        \n",
    "        # Track stats\n",
    "        total_time += result['duration']\n",
    "        if result['success']:\n",
    "            successful += 1\n",
    "        \n",
    "        # Build result entry\n",
    "        entry = {\n",
    "            \"run_id\": run_id,\n",
    "            \"question_index\": i + 1,\n",
    "            \"question_id\": q.get('id', f'Q_{i+1}'),\n",
    "            \"domain\": q.get('domain', ''),\n",
    "            \"category\": q.get('category', ''),\n",
    "            \"difficulty\": q.get('difficulty', 'medium'),\n",
    "            \"test_for\": q.get('test_for', ''),\n",
    "            \"prompt\": prompt,\n",
    "            \"model_tested\": model_name,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"response\": result['response'],\n",
    "            \"metrics\": {\n",
    "                \"response_time_seconds\": result['duration'],\n",
    "                \"response_length_chars\": len(result['response']),\n",
    "                \"response_length_words\": len(result['response'].split())\n",
    "            },\n",
    "            \"execution_success\": result['success'],\n",
    "            \"error_type\": result['error_type']\n",
    "        }\n",
    "        results.append(entry)\n",
    "        \n",
    "        # Show progress\n",
    "        if result['success']:\n",
    "            preview = result['response'][:100] + \"...\" if len(result['response']) > 100 else result['response']\n",
    "            print(f\"âœ“ [{i+1}/{len(questions)}] {q.get('id', 'Q')} ({result['duration']:.2f}s)\")\n",
    "        else:\n",
    "            print(f\"âœ— [{i+1}/{len(questions)}] {q.get('id', 'Q')} - {result['error_type']}\")\n",
    "    \n",
    "    # Build final output\n",
    "    hf_model = backend.get_hf_name(model_name)\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"run_id\": run_id,\n",
    "            \"model\": model_name,\n",
    "            \"hf_model\": hf_model,\n",
    "            \"backend\": {\n",
    "                \"type\": \"transformers\",\n",
    "                \"device\": \"T4 GPU\",\n",
    "                \"use_4bit\": backend.use_4bit\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"key\": dataset_key,\n",
    "                \"total_questions\": len(questions)\n",
    "            },\n",
    "            \"execution\": {\n",
    "                \"platform\": \"Google Colab\",\n",
    "                \"start_time\": results[0]['timestamp'] if results else None,\n",
    "                \"end_time\": datetime.datetime.now().isoformat(),\n",
    "            },\n",
    "            \"statistics\": {\n",
    "                \"total_questions\": len(questions),\n",
    "                \"successful\": successful,\n",
    "                \"failed\": len(questions) - successful,\n",
    "                \"success_rate\": round(successful / len(questions) * 100, 2) if questions else 0,\n",
    "                \"total_response_time\": round(total_time, 2),\n",
    "                \"average_response_time\": round(total_time / len(questions), 3) if questions else 0\n",
    "            }\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    # Save to Drive\n",
    "    clean_model = model_name.replace(':', '_').replace('/', '_')\n",
    "    filename = f\"{dataset_key}_{clean_model}_{run_id}.json\"\n",
    "    filepath = os.path.join(DRIVE_RESULTS_DIR, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… BENCHMARK COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Questions: {len(questions)}\")\n",
    "    print(f\"Success Rate: {output['metadata']['statistics']['success_rate']}%\")\n",
    "    print(f\"Total Time: {total_time:.2f}s\")\n",
    "    print(f\"Avg Time: {output['metadata']['statistics']['average_response_time']:.3f}s\")\n",
    "    print(f\"\\nðŸ“ Results saved to: {filepath}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"âœ“ Benchmark function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model and run benchmark\n",
    "# @title Select Model and Run\n",
    "MODEL = \"qwen2.5:1.5b\"  # @param [\"qwen2.5:0.5b\", \"qwen2.5:1.5b\", \"qwen2.5:3b\", \"smollm2:360m\", \"smollm2:1.7b\", \"phi3.5:3.8b\", \"gemma2:2b\"]\n",
    "DATASET_NAME = \"sample_test\"  # @param {type:\"string\"}\n",
    "\n",
    "# Run the benchmark\n",
    "results = run_benchmark(MODEL, SAMPLE_QUESTIONS, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61ae4f",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a nice format\n",
    "import pandas as pd\n",
    "\n",
    "if 'results' in dir() and results:\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'ID': r['question_id'],\n",
    "            'Domain': r['domain'],\n",
    "            'Time (s)': r['metrics']['response_time_seconds'],\n",
    "            'Words': r['metrics']['response_length_words'],\n",
    "            'Success': 'âœ“' if r['execution_success'] else 'âœ—',\n",
    "            'Response Preview': r['response'][:80] + '...' if len(r['response']) > 80 else r['response']\n",
    "        }\n",
    "        for r in results['results']\n",
    "    ])\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results to display. Run a benchmark first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View individual responses\n",
    "# @title View Response Details\n",
    "QUESTION_INDEX = 0  # @param {type:\"integer\"}\n",
    "\n",
    "if 'results' in dir() and results and QUESTION_INDEX < len(results['results']):\n",
    "    r = results['results'][QUESTION_INDEX]\n",
    "    print(f\"Question ID: {r['question_id']}\")\n",
    "    print(f\"Domain: {r['domain']} / {r['category']}\")\n",
    "    print(f\"Difficulty: {r['difficulty']}\")\n",
    "    print(f\"\\nðŸ“ Prompt:\\n{r['prompt']}\")\n",
    "    print(f\"\\nðŸ¤– Response:\\n{r['response']}\")\n",
    "    print(f\"\\nâ±ï¸ Time: {r['metrics']['response_time_seconds']:.3f}s\")\n",
    "else:\n",
    "    print(\"Invalid index or no results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64382f",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory when done or before loading a different model\n",
    "backend.clear_cache()\n",
    "\n",
    "# Show memory status\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"VRAM: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e736b",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Tips for T4 GPU\n",
    "\n",
    "1. **Model Size**: T4 has 16GB VRAM. Stick to models â‰¤7B for smooth operation.\n",
    "\n",
    "2. **4-bit Quantization**: Enable `USE_4BIT = True` to run larger models (up to 13B).\n",
    "\n",
    "3. **Clear Cache**: Use `backend.clear_cache()` before loading a new model.\n",
    "\n",
    "4. **Gated Models**: For Llama models, you need to:\n",
    "   - Accept the license at huggingface.co/meta-llama\n",
    "   - Login with `login(token=\"your_token\")`\n",
    "\n",
    "5. **Custom Models**: Add your own model mappings:\n",
    "   ```python\n",
    "   backend.model_mappings[\"my_model\"] = \"org/model-name\"\n",
    "   ```\n",
    "\n",
    "6. **Session Limits**: Colab free tier has usage limits. Save results to Drive!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
